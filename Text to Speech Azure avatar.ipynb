{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "873786b9",
   "metadata": {},
   "source": [
    "# Text to Speech avatar\n",
    "\n",
    "## From text to speech with a video avatar provided by Azure Speech Services\n",
    "Custom text to speech avatar allows you to create a customized, one-of-a-kind synthetic talking avatar for your application. With custom text to speech avatar, you can build a unique and natural-looking avatar for your product or brand by providing video recording data of your selected actors. If you also create a custom neural voice for the same actor and use it as the avatar's voice, the avatar will be even more realistic.\n",
    "\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/ai-services/speech-service/text-to-speech-avatar/media/custom-avatar-workflow.png#lightbox\">\n",
    "\n",
    "> https://learn.microsoft.com/en-us/azure/ai-services/speech-service/text-to-speech-avatar/what-is-custom-text-to-speech-avatar \n",
    "> https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-speech-announces-public-preview-of-text-to-speech/ba-p/3981448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install moviepy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca92de3",
   "metadata": {},
   "source": [
    "### 1. `datetime`\n",
    "\n",
    "-   **Purpose**: To work with dates and times.\n",
    "-   **Usage**: Timestamping data, calculating time differences, formatting dates.\n",
    "\n",
    "### 2. `json`\n",
    "\n",
    "-   **Purpose**: To work with JSON (JavaScript Object Notation) data.\n",
    "-   **Usage**: Parsing JSON strings, converting Python objects to JSON format, reading from and writing to JSON files.\n",
    "\n",
    "### 3. `requests`\n",
    "\n",
    "-   **Purpose**: To make HTTP requests.\n",
    "-   **Usage**: Sending HTTP requests to interact with web services (APIs), downloading web content, etc.\n",
    "\n",
    "### 4. `sys`\n",
    "\n",
    "-   **Purpose**: To interact with the Python runtime environment.\n",
    "-   **Usage**: Accessing command-line arguments, handling the interpreter, and system-related functions.\n",
    "\n",
    "### 5. `time`\n",
    "\n",
    "-   **Purpose**: To handle time-related tasks.\n",
    "-   **Usage**: Sleeping, measuring execution time, working with timestamps.\n",
    "\n",
    "### 6. `ipywidgets.Video`\n",
    "\n",
    "-   **Purpose**: To display video widgets in Jupyter notebooks.\n",
    "-   **Usage**: Embedding and controlling video playback within a Jupyter notebook environment.\n",
    "\n",
    "### 7. `IPython.display.display`\n",
    "\n",
    "-   **Purpose**: To display rich output (like videos, images, HTML) in Jupyter notebooks.\n",
    "-   **Usage**: Rendering objects like videos, images, and other rich media in notebook cells.\n",
    "\n",
    "### 8. `moviepy.editor.VideoFileClip`\n",
    "\n",
    "-   **Purpose**: To edit and manipulate video files.\n",
    "-   **Usage**: Reading, editing, and processing video files.\n",
    "\n",
    "### 9. `pathlib.Path`\n",
    "\n",
    "-   **Purpose**: To handle filesystem paths.\n",
    "-   **Usage**: Creating, reading, writing, and manipulating filesystem paths in an object-oriented way.\n",
    "\n",
    "### 10. `logging`\n",
    "\n",
    "-   **Purpose**: To log messages for tracking events that happen during software execution.\n",
    "-   **Usage**: Debugging, error reporting, information logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0441c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from ipywidgets import Video\n",
    "from IPython.display import display\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b49b1",
   "metadata": {},
   "source": [
    "\n",
    "The `sys.version` attribute in Python provides information about the version of Python that is currently being used. This includes the version number, build date, and compiler information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c29b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d691911",
   "metadata": {},
   "source": [
    "1.  **Import the `datetime` module**: This module supplies classes for manipulating dates and times.\n",
    "    \n",
    "2.  **Get the current date and time**:\n",
    "3. **Format the date and time**:\n",
    "The `strftime` method formats the date and time into a string. The format `'%d-%b-%Y %H:%M:%S'` specifies:\n",
    "\n",
    "-   `%d`: Day of the month as a zero-padded decimal number.\n",
    "-   `%b`: Abbreviated month name.\n",
    "-   `%Y`: Year with century as a decimal number.\n",
    "-   `%H`: Hour (24-hour clock) as a zero-padded decimal number.\n",
    "-   `%M`: Minute as a zero-padded decimal number.\n",
    "-   `%S`: Second as a zero-padded decimal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df759539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 15-May-2024 10:36:08\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime.today().strftime('%d-%b-%Y %H:%M:%S')\n",
    "print(f\"Today is {dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed8f63",
   "metadata": {},
   "source": [
    "\n",
    "This function configures the root logger with the specified options:\n",
    "\n",
    "-   **stream=sys.stdout**: Log messages will be sent to standard output (the console).\n",
    "-   **level=logging.INFO**: Only messages with a severity level of INFO or higher will be logged. The levels of severity in ascending order are DEBUG, INFO, WARNING, ERROR, and CRITICAL.\n",
    "-   **format=\"[%(asctime)s] %(message)s\"**: Specifies the format of the log messages. `%(asctime)s` will be replaced with the current time, and `%(message)s` will be replaced with the actual log message.\n",
    "-   **datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\"**: Specifies the format of the date and time in the log messages. This format will display the date and time as MM/DD/YYYY HH:MM:SS AM/PM Timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65f1edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ab2e6",
   "metadata": {},
   "source": [
    "\n",
    "1.  **Set up the Azure Speech service configuration**: You'll need to configure your application to use the provided `azure_speech_key` and `azure_speech_region`.\n",
    "    \n",
    "2.  **Create a Speech Configuration Object**: Use the Azure Speech SDK to create a speech configuration object with your key and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4adb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Speech services informations\n",
    "azure_speech_key = \"bfb7797bd9e74a26b52ddcd253edef56\"\n",
    "azure_speech_region = \"eastus\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544a3c4",
   "metadata": {},
   "source": [
    "The `service_host` variable indicates that you are using a custom endpoint for the Azure Speech service, which suggests that you might be working with Custom Voice or another specialized service within Azure Cognitive Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eef643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_host = \"customvoice.api.speech.microsoft.com\"  # Do not change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bc904",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb6ec8",
   "metadata": {},
   "source": [
    "1.  **Function Definition**: The function `submit_synthesis` is defined to accept one parameter, `prompt`.\n",
    "    \n",
    "2.  **Construct the URL**: The URL for the API endpoint is constructed using formatted string literals (f-strings). This URL combines the `azure_speech_region` and `service_host` to form the base URL for the Azure Speech service, targeting the `batchsynthesis/talkingavatar` endpoint.\n",
    "    \n",
    "3.  **Set up the Headers**: A dictionary named `header` is created to set up the headers for the HTTP POST request. It includes the Azure subscription key (`Ocp-Apim-Subscription-Key`) and specifies that the content type is JSON (`Content-Type`).\n",
    "    \n",
    "4.  **Create the Payload**: A dictionary named `payload` is created to form the payload for the request. This includes:\n",
    "    \n",
    "    -   Metadata about the synthesis job (`displayName` and `description`).\n",
    "    -   Specifies the type of text as plain text (`textType`).\n",
    "    -   Configures the voice to be used for synthesis (`synthesisConfig`).\n",
    "    -   Placeholder for custom voice configurations (commented out in the provided code).\n",
    "    -   The text to be synthesized (`inputs`).\n",
    "    -   Additional properties for the synthesis, such as the avatar character, style, video format, codec, subtitle type, and background color.\n",
    "5.  **Send the POST Request**: The `requests.post` method is used to send an HTTP POST request to the constructed URL with the payload and headers.\n",
    "    \n",
    "6.  **Handle the Response**: The response status code is checked:\n",
    "    \n",
    "    -   If the status code is less than 400 (indicating success), a success message is logged along with the job ID from the response JSON, and the job ID is returned.\n",
    "    -   If the status code is 400 or higher (indicating failure), an error message with the response text is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8be425bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_synthesis(prompt):\n",
    "    url = f\"https://{azure_speech_region}.{service_host}/api/texttospeech/3.1-preview1/batchsynthesis/talkingavatar\"\n",
    "\n",
    "    header = {\n",
    "        \"Ocp-Apim-Subscription-Key\": azure_speech_key,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"displayName\": \"Simple avatar synthesis\",\n",
    "        \"description\": \"Simple avatar synthesis description\",\n",
    "        \"textType\": \"PlainText\",\n",
    "        \"synthesisConfig\": {\n",
    "            \"voice\": \"en-US-JennyNeural\",\n",
    "        },\n",
    "        \"customVoices\": {\n",
    "            # \"YOUR_CUSTOM_VOICE_NAME\": \"YOUR_CUSTOM_VOICE_ID\"\n",
    "        },\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        \"properties\": {\n",
    "            \"customized\": False,  # set to True if you want to use customized avatar\n",
    "            \"talkingAvatarCharacter\": \"lisa\",  # talking avatar character\n",
    "            \"talkingAvatarStyle\": \"graceful-sitting\",  # talking avatar style, required for prebuilt avatar, optional for custom avatar\n",
    "            \"videoFormat\": \"webm\",  # mp4 or webm, webm is required for transparent background\n",
    "            \"videoCodec\": \"vp9\",  # hevc, h264 or vp9, vp9 is required for transparent background; default is hevc\n",
    "            \"subtitleType\": \"soft_embedded\",\n",
    "            \"backgroundColor\": \"transparent\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json.dumps(payload), headers=header)\n",
    "\n",
    "    if response.status_code < 400:\n",
    "        logger.info(\"Batch avatar synthesis job submitted successfully\")\n",
    "        logger.info(f'Job ID: {response.json()[\"id\"]}')\n",
    "        return response.json()[\"id\"]\n",
    "\n",
    "    else:\n",
    "        logger.error(f\"Failed to submit batch avatar synthesis job: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e827f1",
   "metadata": {},
   "source": [
    "1.  **Function Definition**: The function `get_synthesis` is defined to accept one parameter, `job_id`.\n",
    "    \n",
    "2.  **Global Variable**: The function declares that it will use a global variable `avatar_url`.\n",
    "    \n",
    "3.  **Construct the URL**: The URL for the API endpoint is constructed using formatted string literals (f-strings). This URL combines the `azure_speech_region` and `service_host`, and appends the `job_id` to form the complete URL to check the status of the specific batch synthesis job.\n",
    "    \n",
    "4.  **Set up the Headers**: A dictionary named `header` is created to set up the headers for the HTTP GET request. It includes the Azure subscription key (`Ocp-Apim-Subscription-Key`).\n",
    "    \n",
    "5.  **Send the GET Request**: The `requests.get` method is used to send an HTTP GET request to the constructed URL with the headers.\n",
    "    \n",
    "6.  **Handle the Response**: The response status code is checked:\n",
    "    \n",
    "    -   If the status code is less than 400 (indicating success), a debug message is logged indicating the successful retrieval of the batch synthesis job. The JSON response from the server is also logged for debugging purposes.\n",
    "7.  **Check the Job Status**:\n",
    "    \n",
    "    -   The function extracts the `status` from the JSON response.\n",
    "    -   If the `status` is `\"Succeeded\"`, it updates the global variable `avatar_url` with the download URL from the JSON response and logs an info message indicating that the job succeeded along with the download URL.\n",
    "8.  **Return the Status**:\n",
    "    \n",
    "    -   The function returns the `status` of the batch synthesis job.\n",
    "9.  **Error Handling**:\n",
    "    \n",
    "    -   If the status code is 400 or higher (indicating failure), an error message with the response text is logged.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "-   The `get_synthesis` function is designed to retrieve the status of a batch synthesis job submitted to the Azure Speech service using a custom endpoint.\n",
    "-   It dynamically constructs the API endpoint URL based on the provided `job_id`.\n",
    "-   It prepares the necessary headers, sends the GET request, and processes the response.\n",
    "-   If the job has succeeded, it updates a global variable `avatar_url` with the download URL of the synthesized avatar and logs appropriate messages to indicate the status of the job.\n",
    "-   The function handles both successful and failed requests by logging relevant messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9882191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthesis(job_id):\n",
    "    global avatar_url\n",
    "    url = f\"https://{azure_speech_region}.{service_host}/api/texttospeech/3.1-preview1/batchsynthesis/talkingavatar/{job_id}\"\n",
    "\n",
    "    header = {\"Ocp-Apim-Subscription-Key\": azure_speech_key}\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "\n",
    "    if response.status_code < 400:\n",
    "        logger.debug(\"Get batch synthesis job successfully\")\n",
    "        logger.debug(response.json())\n",
    "\n",
    "        status = response.json()[\"status\"]\n",
    "\n",
    "        if status == \"Succeeded\":\n",
    "            avatar_url = response.json()[\"outputs\"][\"result\"]\n",
    "            logger.info(f\"Batch synthesis job succeeded, download URL: {avatar_url}\")\n",
    "\n",
    "        return status\n",
    "    else:\n",
    "        logger.error(f\"Failed to get batch synthesis job: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99614fca",
   "metadata": {},
   "source": [
    "\n",
    "1.  **Function Definition**: The function `list_synthesis_jobs` is defined with two optional parameters, `skip` and `top`, which default to 0 and 100, respectively. These parameters are used for pagination in the API request.\n",
    "    \n",
    "2.  **Construct the URL**: The URL for the API endpoint is constructed using formatted string literals (f-strings). The URL combines the `azure_speech_region` and `service_host` and includes query parameters `skip` and `top` to specify the number of results to skip and the number of results to retrieve, respectively.\n",
    "    \n",
    "3.  **Set up the Headers**: A dictionary named `header` is created to set up the headers for the HTTP GET request. It includes the Azure subscription key (`Ocp-Apim-Subscription-Key`).\n",
    "    \n",
    "4.  **Send the GET Request**: The `requests.get` method is used to send an HTTP GET request to the constructed URL with the headers.\n",
    "    \n",
    "5.  **Handle the Response**: The response status code is checked:\n",
    "    \n",
    "    -   If the status code is less than 400 (indicating success), an info message is logged indicating the successful retrieval of the batch synthesis jobs. The number of jobs retrieved (`len(response.json()[\"values\"])`) is also logged, along with the entire JSON response for additional information.\n",
    "6.  **Error Handling**:\n",
    "    \n",
    "    -   If the status code is 400 or higher (indicating failure), an error message with the response text is logged.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "-   The `list_synthesis_jobs` function is designed to list all batch synthesis jobs in the subscription, allowing for pagination through `skip` and `top` parameters.\n",
    "-   It dynamically constructs the API endpoint URL based on the provided `skip` and `top` values.\n",
    "-   It prepares the necessary headers, sends the GET request, and processes the response.\n",
    "-   If the request is successful, it logs the number of jobs retrieved and the complete JSON response.\n",
    "-   The function handles both successful and failed requests by logging relevant messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49770f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_synthesis_jobs(skip: int = 0, top: int = 100):\n",
    "    \"\"\"List all batch synthesis jobs in the subscription\"\"\"\n",
    "\n",
    "    url = f\"https://{azure_speech_region}.{service_host}/api/texttospeech/3.1-preview1/batchsynthesis/talkingavatar?skip={skip}&top={top}\"\n",
    "\n",
    "    header = {\"Ocp-Apim-Subscription-Key\": azure_speech_key}\n",
    "\n",
    "    response = requests.get(url, headers=header)\n",
    "\n",
    "    if response.status_code < 400:\n",
    "        logger.info(\n",
    "            f'List batch synthesis jobs successfully, got {len(response.json()[\"values\"])} jobs'\n",
    "        )\n",
    "        logger.info(response.json())\n",
    "    else:\n",
    "        logger.error(f\"Failed to list batch synthesis jobs: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb178ad7",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89fc3a",
   "metadata": {},
   "source": [
    "\n",
    "The function `submit_synthesis(prompt)` is designed to submit a text-to-speech synthesis job using Azure Speech Services. Here is a step-by-step explanation of what each part of the code does:\n",
    "\n",
    "1.  **Constructing the URL**: The URL is dynamically generated using the region specified (`azure_speech_region`) and the service host. This URL points to the Azure endpoint for batch synthesis of talking avatars.\n",
    "    \n",
    "2.  **Setting up Headers**: The headers include the subscription key (`azure_speech_key`) and specify that the content type is JSON. These headers are required for authentication and to inform the server about the type of data being sent.\n",
    "    \n",
    "3.  **Creating the Payload**: The payload is a JSON object containing various settings for the synthesis job:\n",
    "    \n",
    "    -   **displayName** and **description**: These provide a name and description for the job.\n",
    "    -   **textType**: Specifies that the input text is plain text.\n",
    "    -   **synthesisConfig**: Contains configuration for the voice used in synthesis.\n",
    "    -   **customVoices**: Placeholder for custom voice settings.\n",
    "    -   **inputs**: An array with the text to be synthesized.\n",
    "    -   **properties**: Various properties for the avatar, such as style, video format, codec, and subtitle type.\n",
    "4.  **Submitting the Request**: The function sends a POST request to the Azure endpoint with the payload and headers.\n",
    "    \n",
    "5.  **Handling the Response**: The function checks the response status code. If the request is successful (status code < 400), it logs a success message along with the job ID. If the request fails, it logs an error message with the response text.\n",
    "\n",
    "\n",
    "The function `get_synthesis(job_id)` is designed to retrieve the status and result of a submitted synthesis job. Here’s the step-by-step explanation:\n",
    "\n",
    "1.  **Constructing the URL**: The URL is built using the job ID to query the specific synthesis job status.\n",
    "    \n",
    "2.  **Setting up Headers**: The headers only include the subscription key for authentication.\n",
    "    \n",
    "3.  **Sending the Request**: The function sends a GET request to the Azure endpoint.\n",
    "    \n",
    "4.  **Handling the Response**: The function checks the response status code. If the request is successful, it logs the status of the job. If the status is \"Succeeded\", it retrieves and logs the URL to download the synthesized avatar. If the request fails, it logs an error message with the response text.\n",
    "\n",
    "\n",
    "The function `list_synthesis_jobs(skip, top)` is designed to list all batch synthesis jobs in the subscription. Here’s the step-by-step explanation:\n",
    "\n",
    "1.  **Constructing the URL**: The URL includes query parameters for skipping a certain number of jobs and limiting the number of jobs returned (`top`).\n",
    "    \n",
    "2.  **Setting up Headers**: The headers include the subscription key for authentication.\n",
    "    \n",
    "3.  **Sending the Request**: The function sends a GET request to the Azure endpoint to list the synthesis jobs.\n",
    "    \n",
    "4.  **Handling the Response**: The function checks the response status code. If the request is successful, it logs the number of jobs retrieved and the response content. If the request fails, it logs an error message with the response text.\n",
    "\n",
    "\n",
    "### Prompt for Avatar Synthesis\n",
    "\n",
    "The prompt text provides a script for the avatar to speak, explaining the Azure OpenAI service. This service provides REST API access to powerful language models like GPT-4 and GPT-3.5-Turbo, which can be used for tasks such as content generation, summarization, semantic search, and natural language to code translation. Microsoft emphasizes responsible AI use, investing in safeguards to prevent abuse and harmful content generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b65c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "I am Lisa, your avatar powered by Azure Speech Services.\n",
    "Today is {dt}.\n",
    "\n",
    "Let me explain you what is Azure Open AI service.\n",
    "\n",
    "Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-4, GPT-3.5-Turbo, and Embeddings model series. In addition, the new GPT-4 and GPT-3.5-Turbo model series have now reached general availability. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio.\n",
    "\n",
    "At Microsoft, we're committed to the advancement of AI driven by principles that put people first. Generative models such as the ones available in Azure OpenAI have significant potential benefits, but without careful design and thoughtful mitigations, such models have the potential to generate incorrect or even harmful content. Microsoft has made significant investments to help guard against abuse and unintended harm, which includes requiring applicants to show well-defined use cases, incorporating Microsoft’s principles for responsible AI use, building content filters to support customers, and providing responsible AI implementation guidance to onboarded customers.\n",
    "\n",
    "To learn more go to https://azure.microsoft.com/en-us/products/ai-services/ai-speech\n",
    "\n",
    "Thank you and have a good day.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c824a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am Lisa, your avatar powered by Azure Speech Services.\n",
      "Today is 15-May-2024 10:32:50.\n",
      "\n",
      "Let me explain you what is Azure Open AI service.\n",
      "\n",
      "Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-4, GPT-3.5-Turbo, and Embeddings model series. In addition, the new GPT-4 and GPT-3.5-Turbo model series have now reached general availability. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio.\n",
      "\n",
      "At Microsoft, we're committed to the advancement of AI driven by principles that put people first. Generative models such as the ones available in Azure OpenAI have significant potential benefits, but without careful design and thoughtful mitigations, such models have the potential to generate incorrect or even harmful content. Microsoft has made significant investments to help guard against abuse and unintended harm, which includes requiring applicants to show well-defined use cases, incorporating Microsoft’s principles for responsible AI use, building content filters to support customers, and providing responsible AI implementation guidance to onboarded customers.\n",
      "\n",
      "To learn more go to https://azure.microsoft.com/en-us/products/ai-services/ai-speech\n",
      "\n",
      "Thank you and have a good day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b315fc2",
   "metadata": {},
   "source": [
    "## Avatar batch generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbb4e0",
   "metadata": {},
   "source": [
    "\n",
    "1.  **Start Timer**: The code begins by recording the current time to later calculate how long the process takes.\n",
    "    \n",
    "2.  **Submit Synthesis Job**: It calls the `submit_synthesis` function with a prompt containing detailed information about Azure OpenAI services. The function returns a job ID, which is stored for tracking the job status.\n",
    "    \n",
    "3.  **Check Job ID**: The script verifies if the job ID is not `None`. If the job submission fails and returns `None`, the process stops here.\n",
    "    \n",
    "4.  **Polling Loop**: The code enters an infinite loop to repeatedly check the status of the synthesis job.\n",
    "    \n",
    "5.  **Get Job Status**: It calls the `get_synthesis` function with the job ID to retrieve the current status of the job.\n",
    "    \n",
    "6.  **Check Status**:\n",
    "    \n",
    "    -   **Succeeded**: If the job status is \"Succeeded\", the code logs a success message, calculates the total elapsed time, and prints it. The loop then breaks to stop further checks.\n",
    "    -   **Failed**: If the job status is \"Failed\", an error message is logged, and the loop breaks.\n",
    "    -   **In Progress**: If the job is still in progress, it logs the current status and waits for 30 seconds before checking again. This prevents excessive requests to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d69fbed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/15/2024 10:36:31 AM India Standard Time] Batch avatar synthesis job submitted successfully\n",
      "[05/15/2024 10:36:31 AM India Standard Time] Job ID: df6346b1-7533-49f6-a06e-0c631721c11e\n",
      "[05/15/2024 10:36:33 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:37:05 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:37:36 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:38:08 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:38:39 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:39:11 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:39:42 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:40:14 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:40:46 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:41:17 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:41:49 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:42:21 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:42:52 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:43:24 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:43:55 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:44:27 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:45:05 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:45:37 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:46:08 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:46:40 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:47:12 AM India Standard Time] Please wait. Status: [Running]\n",
      "[05/15/2024 10:47:44 AM India Standard Time] Batch synthesis job succeeded, download URL: https://cvoiceprodeus.blob.core.windows.net/batch-synthesis-output/df6346b1-7533-49f6-a06e-0c631721c11e/0001.webm?skoid=85130dbe-2390-4897-a9e9-5c88bb59daff&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skt=2024-05-15T05%3A12%3A43Z&ske=2024-05-21T05%3A17%3A43Z&sks=b&skv=2023-11-03&sv=2023-11-03&st=2024-05-15T05%3A12%3A43Z&se=2024-05-16T05%3A17%3A43Z&sr=b&sp=rl&sig=Jb1HtDR%2FB0Pa0cqUa7Qu%2B1Gq2cFYJ8SIUtY6WEOn0o0%3D\n",
      "[05/15/2024 10:47:44 AM India Standard Time] Done! Azure batch avatar synthesis job succeeded.\n",
      "Elapsed time: 00:11:13.890464\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "job_id = submit_synthesis(prompt)\n",
    "\n",
    "if job_id is not None:\n",
    "    while True:\n",
    "        status = get_synthesis(job_id)\n",
    "        if status == \"Succeeded\":\n",
    "            logger.info(\"Done! Azure batch avatar synthesis job succeeded.\")\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Elapsed time: \" + time.strftime(\"%H:%M:%S.{}\".format(str(elapsed % 1)[2:])[:15],\n",
    "                                                   time.gmtime(elapsed)))\n",
    "\n",
    "            break\n",
    "        elif status == \"Failed\":\n",
    "            logger.error(\"Failed\")\n",
    "            break\n",
    "        else:\n",
    "            logger.info(f\"Please wait. Status: [{status}]\")\n",
    "            time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b22e3",
   "metadata": {},
   "source": [
    "## Avatar video file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac075f52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "973f2119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31;34mThis is the prompt to speak:\n",
      " \n",
      "I am Lisa, your avatar powered by Azure Speech Services.\n",
      "Today is 15-May-2024 10:36:08.\n",
      "\n",
      "Let me explain you what is Azure Open AI service.\n",
      "\n",
      "Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-4, GPT-3.5-Turbo, and Embeddings model series. In addition, the new GPT-4 and GPT-3.5-Turbo model series have now reached general availability. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio.\n",
      "\n",
      "At Microsoft, we're committed to the advancement of AI driven by principles that put people first. Generative models such as the ones available in Azure OpenAI have significant potential benefits, but without careful design and thoughtful mitigations, such models have the potential to generate incorrect or even harmful content. Microsoft has made significant investments to help guard against abuse and unintended harm, which includes requiring applicants to show well-defined use cases, incorporating Microsoft’s principles for responsible AI use, building content filters to support customers, and providing responsible AI implementation guidance to onboarded customers.\n",
      "\n",
      "To learn more go to https://azure.microsoft.com/en-us/products/ai-services/ai-speech\n",
      "\n",
      "Thank you and have a good day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\033[1;31;34mThis is the prompt to speak:\\n {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28b67132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/15/2024 11:02:09 AM India Standard Time] c:\\Users\\ParveenKR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:123: UserWarning: Warning: in file https://cvoiceprodeus.blob.core.windows.net/batch-synthesis-output/df6346b1-7533-49f6-a06e-0c631721c11e/0001.webm?skoid=85130dbe-2390-4897-a9e9-5c88bb59daff&sktid=33e01921-4d64-4f8c-a055-5bdaffd5e33d&skt=2024-05-15T05%3A12%3A43Z&ske=2024-05-21T05%3A17%3A43Z&sks=b&skv=2023-11-03&sv=2023-11-03&st=2024-05-15T05%3A12%3A43Z&se=2024-05-16T05%3A17%3A43Z&sr=b&sp=rl&sig=Jb1HtDR%2FB0Pa0cqUa7Qu%2B1Gq2cFYJ8SIUtY6WEOn0o0%3D, 6220800 bytes wanted but 0 bytes read,at frame 2686/2687, at time 107.44/107.45 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save avatar video\n",
    "\n",
    "avatar_file = (\n",
    "    \"azure_avatar_\" + str(datetime.datetime.today().strftime(\"%d%b%Y_%H%M%S\")) + \".mp4\"\n",
    ")\n",
    "VideoFileClip(avatar_url).write_videofile(avatar_file, verbose=False, logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4275e901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8097f27628f84358b8b5450b2b523b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing the avatar video\n",
    "\n",
    "Video.from_file(avatar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad125c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
